{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e44f8ff1-ee4d-452e-8f9f-d36fdf9e9d3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Processing... data/mah.pdf\n",
      "================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## \t Education\t\n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \n",
      "\n",
      "## \t Education\t\n",
      "\n",
      "  ‚óè    B.Tech( Computer Science and Engineering)  -8.34 CGPA   (2020) \n",
      "\n",
      "Predicted page in DOCTAGS:\n",
      "Assistant: <doctag><section_header_level_1><loc_58><loc_63><loc_225><loc_77>Kapuluru Mahendra Reddy</section_header_level_1>\n",
      "<text><loc_58><loc_80><loc_196><loc_90>Machine Learning Engineer</text>\n",
      "<text><loc_58><loc_93><loc_236><loc_102>www.linkedin.com/in/mahendrareddykapuluru</text>\n",
      "<section_header_level_1><loc_58><loc_118><loc_169><loc_127>Professional Summary</section_header_level_1>\n",
      "<unordered_list><list_item><loc_74><loc_135><loc_437><loc_163>¬∑ Results-driven ml Engineer with 4+ years of experience in automation, data science, machine learning , and networking solutions. Strong background in designing end-to-end ML solutions, MLOps , and cybersecurity threat detection.</list_item>\n",
      "<list_item><loc_74><loc_164><loc_437><loc_192>¬∑ Expertise in Python automation, data engineering, and scalable machine learning models and MLOPS with hands-on experience in MongoDB, Airflow, MLflow, Terraform, Docker, FastAPI, and AWS.</list_item>\n",
      "<list_item><loc_74><loc_192><loc_425><loc_219>¬∑ Built an end-to-end cybersecurity threat detection system, integrating automated ETL pipelines, real-time API endpoints, model drift detection, and MLOPS CI/CD workflows.</list_item>\n",
      "<list_item><loc_74><loc_220><loc_432><loc_238>¬∑ Proven ability to improve operational efficiency, including a 40% increase in data consistency and a 50% reduction in incident resolution time through automation.</list_item>\n",
      "<list_item><loc_74><loc_239><loc_433><loc_257>¬∑ Strong experience in API and database integrations with platforms such as ServiceNow, CMDB, Oracle SQL, and Python-based data transformation pipelines.</list_item>\n",
      "<list_item><loc_74><loc_258><loc_422><loc_276>¬∑ Hands-on experience in real-time network monitoring using BGP, MLAG, NetFlow, and SNMP for proactive issue detection.</list_item>\n",
      "<list_item><loc_74><loc_277><loc_424><loc_295>¬∑ Developed and maintained machine learning models, improving accuracy by up to 20% with precise data labeling and annotation workflows.</list_item>\n",
      "<list_item><loc_74><loc_296><loc_417><loc_314>¬∑ Skilled in infrastructure automation with Ansible, NetBrain, Docker and Terraform, ensuring scalability and reliability in deployments.</list_item>\n",
      "<list_item><loc_74><loc_315><loc_430><loc_333>¬∑ Experienced in leading end-to-end automation and integration projects, collaborating with cross-functional teams to deliver scalable, efficient solutions.</list_item>\n",
      "</unordered_list>\n",
      "<section_header_level_1><loc_58><loc_341><loc_113><loc_350>Experience</section_header_level_1>\n",
      "<section_header_level_1><loc_67><loc_358><loc_194><loc_366>TATA CONSULTANCY SERVICES</section_header_level_1>\n",
      "<text><loc_67><loc_370><loc_273><loc_378>Client: United Services Automobile Association</text>\n",
      "<section_header_level_1><loc_67><loc_383><loc_110><loc_391>Project 1:</section_header_level_1>\n",
      "<text><loc_58><loc_395><loc_440><loc_427>Developed an end-to-end machine learning-powered cybersecurity threat detection system, integrating ETL pipelines, model development, deployment, monitoring, and MLOps best practices.</text>\n",
      "<text><loc_271><loc_420><loc_347><loc_428>2024 June - Present</text>\n",
      "<text><loc_74><loc_439><loc_153><loc_447>Key Contributions:</text>\n",
      "<text><loc_294><loc_358><loc_381><loc_366>2021 Feb - Present</text>\n",
      "</doctag><end_of_utterance>\n",
      "\n",
      "Predicted page in DOCTAGS:\n",
      "Assistant: <doctag><list_item><loc_73><loc_47><loc_437><loc_55>¬∑ Designed and implemented ETL pipelines for automated data ingestion using MongoDB.</list_item>\n",
      "<list_item><loc_73><loc_56><loc_422><loc_73>¬∑ Conducted data analysis, visualization, transformation, model training, evaluation, and prediction.</list_item>\n",
      "<list_item><loc_73><loc_74><loc_367><loc_83>¬∑ Developed a modular pipeline for scalability and easy maintainability.</list_item>\n",
      "<list_item><loc_73><loc_84><loc_408><loc_92>¬∑ Integrated Amazon S3 for storing artifacts, ensuring efficient data management.</list_item>\n",
      "<list_item><loc_73><loc_93><loc_427><loc_111>¬∑ Created modular skeleton and structured data validation schemas to maintain data integrity.</list_item>\n",
      "<list_item><loc_73><loc_112><loc_367><loc_120>¬∑ Configured logging mechanisms for process tracking and debugging.</list_item>\n",
      "<list_item><loc_73><loc_121><loc_438><loc_129>¬∑ Built CI/CD pipelines using GitHub Actions for seamless model deployment and updates.</list_item>\n",
      "<list_item><loc_73><loc_130><loc_355><loc_138>¬∑ Deployed RESTful APIs using FastAPI for real-time threat prediction.</list_item>\n",
      "<list_item><loc_73><loc_139><loc_433><loc_147>¬∑ Managed infrastructure as code (IaC) using Terraform for reproducibility and scalability.</list_item>\n",
      "<list_item><loc_73><loc_148><loc_407><loc_156>¬∑ Automated pipeline scheduling with Apache Airflow for streamlined execution.</list_item>\n",
      "<list_item><loc_73><loc_157><loc_437><loc_165>¬∑ Implemented ML monitoring using MLflow to track model performance and degradation.</list_item>\n",
      "<list_item><loc_73><loc_166><loc_417><loc_184>¬∑ Monitored model drift using Kullback-Leibler (KL) Divergence , enabling proactive retraining</list_item>\n",
      "</unordered_list>\n",
      "<text><loc_59><loc_194><loc_431><loc_211>Tools & Technologies: Python, MongoDB, scikit-learn, Airflow, MLflow, Terraform, Docker, FastAPI, Amazon AWS, GitHub Actions</text>\n",
      "<section_header_level_1><loc_70><loc_220><loc_113><loc_228>Project 2:</section_header_level_1>\n",
      "<text><loc_70><loc_240><loc_270><loc_248>Network Traffic Analysis and Anomaly Detection</text>\n",
      "<text><loc_309><loc_240><loc_395><loc_248>2024 Feb - 2024 May</text>\n",
      "<unordered_list><list_item><loc_73><loc_260><loc_429><loc_287>¬∑ Designed and deployed a real-time anomaly detection system for BGP route advertisements and MLAG link monitoring, combining Python-based automation with SQL-driven data analysis.</list_item>\n",
      "<list_item><loc_73><loc_288><loc_440><loc_306>¬∑ Created and optimized SQL tables and queries to store, aggregate, and analyze large volumes of routing updates and NetFlow data for enhanced network monitoring.</list_item>\n",
      "<list_item><loc_73><loc_307><loc_429><loc_325>¬∑ Integrated SNMP logs and traffic metrics into SQL-based reports , enabling trend analysis and anomaly correlation.</list_item>\n",
      "<list_item><loc_73><loc_326><loc_429><loc_344>¬∑ Developed Python scripts to identify irregular network behaviors, triggering automated alerts and escalating critical issues.</list_item>\n",
      "</unordered_list>\n",
      "<section_header_level_1><loc_73><loc_352><loc_137><loc_360>Achievements:</section_header_level_1>\n",
      "<unordered_list><list_item><loc_73><loc_369><loc_422><loc_386>¬∑ Reduced network downtime by 30% through proactive detection of route changes, link failures, and network disruptions.</list_item>\n",
      "<list_item><loc_73><loc_387><loc_422><loc_395>¬∑ Enhanced overall network stability and response time with early warning mechanisms.</list_item>\n",
      "</unordered_list>\n",
      "<text><loc_73><loc_404><loc_329><loc_412>Tools & Technologies: Python, BGP, MLAG , SQL, NetFlow, SNMP</text>\n",
      "<section_header_level_1><loc_73><loc_422><loc_114><loc_430>Project 3:</section_header_level_1>\n",
      "<text><loc_73><loc_442><loc_279><loc_450>Automated Network Data Collection & Integration</text>\n",
      "<text><loc_313><loc_442><loc_391><loc_450>2022 Jan - 2024 Feb</text>\n",
      "</doctag><end_of_utterance>\n",
      "\n",
      "Predicted page in DOCTAGS:\n",
      "Assistant: <doctag><list_item><loc_73><loc_47><loc_391><loc_64>¬∑ Developed Python scripts to collect BGP, MLAG , and interface details, ensuring comprehensive network monitoring.</list_item>\n",
      "<list_item><loc_73><loc_65><loc_418><loc_83>¬∑ Scheduled automated jobs to execute at regular intervals, ensuring timely updates and proactive issue detection.</list_item>\n",
      "<list_item><loc_73><loc_84><loc_441><loc_102>¬∑ Designed interactive dashboards to visualize collected data, enhancing network analysis and troubleshooting.</list_item>\n",
      "<list_item><loc_73><loc_103><loc_418><loc_121>¬∑ Integrated NetBrain with Ansible to automate network operations and configuration management.</list_item>\n",
      "<list_item><loc_73><loc_122><loc_427><loc_139>¬∑ Integrated NetBrain with ServiceNow, enabling automatic ticket updates with real-time network insights.</list_item>\n",
      "</unordered_list>\n",
      "<section_header_level_1><loc_70><loc_147><loc_133><loc_156>Achievements:</section_header_level_1>\n",
      "<unordered_list><list_item><loc_73><loc_164><loc_431><loc_182>¬∑ Reduced network incident resolution time by 50% by automating root cause analysis and monitoring workflows.</list_item>\n",
      "<list_item><loc_73><loc_183><loc_441><loc_201>¬∑ Minimized manual effort through process automation, freeing up resources for high-priority tasks.</list_item>\n",
      "</unordered_list>\n",
      "<text><loc_70><loc_209><loc_383><loc_218>Tools & Technologies: Python, Ansible, SQL, Oracle SQL, NetBrain, ServiceNow</text>\n",
      "<section_header_level_1><loc_70><loc_245><loc_114><loc_254>Project 4:</section_header_level_1>\n",
      "<text><loc_70><loc_265><loc_196><loc_274>Data Labeling and Annotation</text>\n",
      "<text><loc_309><loc_265><loc_396><loc_274>2021 April - December</text>\n",
      "<unordered_list><list_item><loc_73><loc_284><loc_418><loc_302>¬∑ Labeled datasets using Ubiai tool, ensuring precision and high-quality annotations for machine learning model training.</list_item>\n",
      "<list_item><loc_73><loc_303><loc_424><loc_321>¬∑ Developed and maintained a systematic approach for data labeling, reducing errors and improving efficiency.</list_item>\n",
      "<list_item><loc_73><loc_322><loc_431><loc_340>¬∑ Integrated labeled data into machine learning pipelines , enhancing model accuracy and robustness.</list_item>\n",
      "</unordered_list>\n",
      "<section_header_level_1><loc_73><loc_349><loc_137><loc_358>Achievements:</section_header_level_1>\n",
      "<unordered_list><list_item><loc_73><loc_366><loc_436><loc_384>¬∑ Boosted machine learning model accuracy by 15%-20% through well-structured, properly annotated training data.</list_item>\n",
      "<list_item><loc_73><loc_385><loc_420><loc_403>¬∑ Enhanced data retrieval efficiency, reducing processing time for feature extraction and training iterations.</list_item>\n",
      "</unordered_list>\n",
      "<text><loc_73><loc_411><loc_236><loc_420>Tools: Ubiai, Python,Machine Learning</text>\n",
      "<section_header_level_1><loc_70><loc_428><loc_139><loc_437>Skills Summary :</section_header_level_1>\n",
      "</doctag><end_of_utterance>\n",
      "\n",
      "Predicted page in DOCTAGS:\n",
      "Assistant: <doctag><list_item><loc_73><loc_47><loc_433><loc_74>¬∑ Core Expertise: Python Automation, Machine Learning, Data Science, Deep Learning,NLP, Transformers and Predictive Modeling, Computer Vision, Networking, API Integration ,LLMs, Gen AI</list_item>\n",
      "<list_item><loc_73><loc_75><loc_231><loc_83>¬∑ Programming Languages: Python C</list_item>\n",
      "<list_item><loc_73><loc_84><loc_424><loc_101>¬∑ Frameworks & Libraries: TensorFlow, Scikit-learn, NumPy, Pandas, Matplotlib,FastAPI ,LlamalIndex</list_item>\n",
      "<list_item><loc_73><loc_102><loc_426><loc_110>¬∑ Networking & Automation Tools: NetBrain, Ansible, BGP, MLAG, SNMP, NetFlow, sFlow</list_item>\n",
      "<list_item><loc_73><loc_111><loc_321><loc_120>¬∑ Tools: Oracle SQL, Docker, Git,Mflow, Airflow,Github-Actions</list_item>\n",
      "<list_item><loc_73><loc_121><loc_424><loc_138>¬∑ Soft Skills: Problem Solving, Analytical Thinking, Team Collaboration, Leadership, Time Management</list_item>\n",
      "<list_item><loc_73><loc_139><loc_231><loc_148>¬∑ Cloud: AWS EC2, ECR, S3,Sage Maker</list_item>\n",
      "</unordered_list>\n",
      "<section_header_level_1><loc_66><loc_157><loc_106><loc_165>Projects.</section_header_level_1>\n",
      "<unordered_list><list_item><loc_73><loc_174><loc_303><loc_182>¬∑ Retrieval-Augmented Question Answering System</list_item>\n",
      "<list_item><loc_103><loc_183><loc_373><loc_209>o Developed an interactive question-answering application utilizing Retrieval-Augmented Generation (RAG), integrating LlamalIndex for document indexing and Google's Gemini for language modeling.</list_item>\n",
      "<list_item><loc_103><loc_210><loc_426><loc_236>o Engineered a robust pipeline to process custom documents, employing advanced techniques for parsing, chunking, embedding, and retrieval to ensure accurate context extraction.</list_item>\n",
      "<list_item><loc_103><loc_237><loc_404><loc_253>o Designed a user-friendly interface with Streamlit, enabling users to upload documents and receive real-time, context-aware responses to their queries</list_item>\n",
      "<list_item><loc_103><loc_254><loc_307><loc_262>o Tools: Python, Scikit-learn, Pandas, Matplotlib</list_item>\n",
      "<list_item><loc_73><loc_263><loc_301><loc_271>¬∑ Heart Disease Prediction using Machine Learning</list_item>\n",
      "<list_item><loc_103><loc_272><loc_440><loc_298>o Developed a predictive model for heart disease detection using supervised learning techniques, implementing KNN, Decision Trees, and Logistic Regression to classify patient data with optimized accuracy.</list_item>\n",
      "<list_item><loc_103><loc_299><loc_307><loc_307>o Tools: Python, Scikit-learn, Pandas, Matplotlib</list_item>\n",
      "<list_item><loc_73><loc_308><loc_287><loc_317>¬∑ Spoken Digit Recognition using Deep Learning</list_item>\n",
      "<list_item><loc_103><loc_318><loc_435><loc_334>o Built an LSTM-based model for spoken digit recognition, utilizing spectrograms for feature extraction and data augmentation to improve generalization.</list_item>\n",
      "<list_item><loc_103><loc_335><loc_289><loc_343>o Tools: Python, TensorFlow, Keras, Librosa</list_item>\n",
      "<list_item><loc_73><loc_344><loc_294><loc_353>¬∑ Amazon Review Sentiment Analysis using BERT</list_item>\n",
      "<list_item><loc_103><loc_354><loc_415><loc_380>o Developed a BERT-based sentiment classification system for Amazon reviews, fine-tuning pre-trained embeddings and comparing results with TF-IDF and Word2Vec approaches.</list_item>\n",
      "<list_item><loc_103><loc_381><loc_362><loc_390>o Tools: Python, NLP, Hugging Face Transformers, TensorFlow</list_item>\n",
      "<list_item><loc_73><loc_391><loc_282><loc_400>¬∑ Image Classification using Transfer Learning</list_item>\n",
      "<list_item><loc_103><loc_401><loc_440><loc_420>o Implemented a VGG-16-based CNN model for image classification, fine-tuning layers on a custom dataset and using data augmentation for accuracy improvement.</list_item>\n",
      "<list_item><loc_103><loc_421><loc_323><loc_429>o Tools: Python, TensorFlow, OpenCV, VGG-16, Keras</list_item>\n",
      "</unordered_list>\n",
      "<section_header_level_1><loc_66><loc_437><loc_123><loc_445>Certifications</section_header_level_1>\n",
      "</doctag><end_of_utterance>\n",
      "\n",
      "Predicted page in DOCTAGS:\n",
      "Assistant: <doctag><list_item><loc_73><loc_47><loc_250><loc_54>¬∑ Certified in Problem Solving | HackerRank</list_item>\n",
      "<list_item><loc_73><loc_57><loc_258><loc_64>¬∑ Certified in SQL Programming | HackerRank</list_item>\n",
      "</unordered_list>\n",
      "<section_header_level_1><loc_64><loc_74><loc_108><loc_82>Education</section_header_level_1>\n",
      "<unordered_list><list_item><loc_73><loc_91><loc_335><loc_99>¬∑ B.Tech( Computer Science and Engineering) -8.34 CGPA (2020)</list_item>\n",
      "</unordered_list>\n",
      "</doctag><end_of_utterance>\n",
      "\n",
      "Total document prediction time: 3062.68 seconds, pages: 5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorDevice,\n",
    "    VlmPipelineOptions,\n",
    "    granite_vision_vlm_conversion_options,\n",
    "    smoldocling_vlm_conversion_options,\n",
    "    smoldocling_vlm_mlx_conversion_options,\n",
    ")\n",
    "from docling.datamodel.settings import settings\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "sources = [\n",
    "    # \"tests/data/2305.03393v1-pg9-img.png\",\n",
    "    \"data/mah.pdf\",\n",
    "]\n",
    "\n",
    "## Use experimental VlmPipeline\n",
    "pipeline_options = VlmPipelineOptions()\n",
    "# If force_backend_text = True, text from backend will be used instead of generated text\n",
    "pipeline_options.force_backend_text = True\n",
    "\n",
    "## Pick a VLM model. Fast Apple Silicon friendly implementation for SmolDocling-256M via MLX\n",
    "pipeline_options.vlm_options = smoldocling_vlm_conversion_options\n",
    "\n",
    "#pipeline_options.vlm_options = granite_vision_vlm_conversion_options\n",
    "from docling_core.types.doc import DocItemLabel, ImageRefMode\n",
    "from docling_core.types.doc.document import DEFAULT_EXPORT_LABELS\n",
    "\n",
    "## Set up pipeline for PDF or image inputs\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_cls=VlmPipeline,\n",
    "            pipeline_options=pipeline_options,\n",
    "        ),\n",
    "        InputFormat.IMAGE: PdfFormatOption(\n",
    "            pipeline_cls=VlmPipeline,\n",
    "            pipeline_options=pipeline_options,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "out_path = Path(\"scratch\")\n",
    "out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for source in sources:\n",
    "    start_time = time.time()\n",
    "    print(\"================================================\")\n",
    "    print(\"Processing... {}\".format(source))\n",
    "    print(\"================================================\")\n",
    "    print(\"\")\n",
    "\n",
    "    res = converter.convert(source)\n",
    "    print(\"\")\n",
    "    print(res.document.export_to_markdown())\n",
    "    res.document.save_as_html(\n",
    "        filename=Path(\"{}/{}.html\".format(out_path, res.input.file.stem)),\n",
    "        image_mode=ImageRefMode.REFERENCED,\n",
    "        labels=[*DEFAULT_EXPORT_LABELS, DocItemLabel.FOOTNOTE],\n",
    "    )\n",
    "    res.document.save_as_markdown(\n",
    "        out_path / f\"{res.input.file.stem}.md\",\n",
    "        image_mode=ImageRefMode.PLACEHOLDER,\n",
    "    )\n",
    "\n",
    "    pg_num = res.document.num_pages()\n",
    "    print(\"\")\n",
    "    inference_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"Total document prediction time: {inference_time:.2f} seconds, pages: {pg_num}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c51296-128d-4e04-9e05-defc7d02650a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327b0ad2-6e3b-455c-98c8-8be6be9a19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "603558c7-fe6a-4920-9b29-f75103ff11bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00b5a00e-607d-4105-a43d-ab90c0078057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Mohan Reddy Pallavula\n",
      "linkedin/mohanreddypallavula github/mohanreddypallavula\n",
      "Experience\n",
      "## ¬∑ Matdun Labs India Pvt. Ltd\n",
      "Remote\n",
      "Dec 2021 - present\n",
      "- AI Engineer (Full-time)\n",
      "- ‚ó¶ Face Recognition system : Developed an advanced face recognition system utilizing SOTA based models for detecting and recognition the face and deployed on edge device (jetson nano) and kubernetes cluster (Nvidia Gpus). Optimized the models using tensorrt to reduce latency and Used Nvidia Triton server for dynamic batching, GPU and CPU optimization, and robust scalability, ensuring efficient and high-performance inference for deployed AI models. Tech: Tensorrt , Nvidia Jetson Nano , Kubernetes , Docker , FastAPI , Django , Web sockets , Grpc ,Pytorch , Opencv , Scikit-learn , PostgresSQL , Azure blob , GIT\n",
      "- ‚ó¶ Video Analytics System : Delevoped an advanced AI-powered solution designed for real-time monitoring and analysis of video streams. It offers features such as person tracking, which enables precise identification and movement analysis, and person analytics, including metrics like waiting time and behavior patterns. The system incorporates specialized detection capabilities, such as weapon detection for enhanced security and fall detection for safety monitoring in environments.\n",
      "- Tech: Tensorrt , Kubernetes , Docker , FastAPI , Redis , Grpc , Pytorch Lightning , Litdata , Opencv , Scikit-learn , Nvidia Trition Server , Nvidia deepstream , Pytorch , MLflow , GIT , PostgresSQL , Azure blob\n",
      "- ‚ó¶ Generate Description based on activities : Automatically generate detailed and contextually accurate descriptions of activities. We used few shot prompting and llama 3 model to develop the system. Tech: LitGPT , pytorch lightning , Litserve , FastAPI , Redis , QLora , Azure blob , Langchain , Langgraph , Azure Cloud , OpenAI API(GPT-4o) , Llama 3 , RAG\n",
      "## ¬∑ Capillary Technologies Pvt Ltd\n",
      "Remote\n",
      "June 2021 - Nov 2021\n",
      "- ML Intern (Full-time)\n",
      "- ‚ó¶ Smart Store : Developed an advanced deep learning to detect the person age group , identifies fashion types, such as specific dress styles.\n",
      "- Developed person tracking to analyze customer behavior within the store. By tracking where individuals spend their time, the system provides insights into high-traffic areas and zones of interest.\n",
      "- Tech: Deep Learning , AWS , Computer Vision , Nodejs , Docker , GIT\n",
      "## ¬∑ Appcilious Pvt. Ltd\n",
      "Remote\n",
      "Sep 2020 - Nov 2020\n",
      "- ML Intern (Full-time)\n",
      "- ‚ó¶ Fake News Classification : Developed an advanced text analysis techniques to accurately identify and categorize misinformation. It employs TF-IDF for initial text feature extraction, Word2Vec for capturing semantic word relationships, and transformer-based embeddings, specifically the Universal Sentence Encoder, for deep contextual understanding. By combining these methods, the system enhances its ability to discern fake news from credible sources. Tech: Machine Learning , NLTK , Scikit-learn , Tf-idf , Deep learning , word2vec.\n",
      "## Skills Summary\n",
      "- ¬∑ Core Expertise : Data Science, Machine Learning, Deep Learning, Natural Language Processing (NLP), Computer Vision, Predictive Modeling, Decision Analytics, Large Language Models (LLMs), Generative AI\n",
      "- ¬∑ Languages :\n",
      "Python, C , C++\n",
      "- ¬∑ Frameworks : Pytorch , Scikit-learn , FastAPI , Django, Flask , Nvidia Deepestream , Nvidia Triton Server , Langgraph , Langchain , Litgpt , Litdata , Pytorch lightning , Huggingface libraries ( Transformers , peft)\n",
      "- ¬∑ Tools :\n",
      "Kubernetes, Docker, GIT, PostgreSQL, Redis , Gitlab CI-CD\n",
      "- ¬∑ Platforms : Linux, Web, Windows, Nvidia Jetson Nano, Raspberry , Azure , Basic Aws\n",
      "- ¬∑ Soft Skills :\n",
      "Leadership, Time Management\n",
      "Projects\n",
      "- ¬∑ Implemented Tinyllama from scratch in pytorch : Built a lightweight version of the LLaMA language model from the ground up using PyTorch, focusing on replicating core transformer architecture with an emphasis on model efficiency and size reduction. Loaded the pretrained weights into our architecture and implemented inference pipeline. Implemented client and server using grpc python.\n",
      "- Tech: PyTorch, Transformers, Attention Mechanisms, Deep Learning , KV cache , Grpc , LLM, GenAI .\n",
      "- ¬∑ Image Captioning using Deep Learning (NLP, CV) : Developed an advanced image captioning system using an encoder-decoder architecture to automatically generate coherent and contextually relevant textual descriptions from images. Tech: Python, Tensorflow, Streamlit , and Deep Learning\n",
      "- ¬∑ Semantic search engine on stackoverflow python data (NLP) : Developed a robust semantic search engine designed to enhance search accuracy and relevance within StackOverflow Python data by leveraging advanced natural language processing techniques.\n",
      "- Tech: Python, Natural Language Processing (NLP), Word Embeddings, TF-IDF, Transformer Models, Elasticsearch, Scikit-learn, and Pandas.\n",
      "- ¬∑ Face Recognition using Deep Learning : Developed an advanced face recognition system utilizing deep learning techniques to accurately identify and verify individuals from images, with applications in security, identity verification. Tech: Python, Pytorch, Convolutional Neural Networks (CNN), OpenCV, Scikit-learn, TensorRT , Nvidia Jetson nano , fastapi , and Web sockets.\n",
      "Mobile:\n",
      "+91-8309913459\n",
      "Email: mohanreddy.pmg@gmail.com\n",
      "‚Ä¢ Bachelor of Technology - Computer Science and Engineering; GPA: 8.0\n",
      "## JNTUA Engineering College\n",
      "Kalikiri , AP , India\n",
      "July 2017 - June 2021\n",
      "‚Ä¢ Intermediate - MPC; Per: 96.7\n",
      "Sri Chaitanya Junior College\n",
      "Tirupati , AP , India\n",
      "July 2015 - June 2017\n",
      "‚Ä¢ AP Residential School\n",
      "Gyaram Palli , AP , India\n",
      "SSC ; GPA: 9.5\n",
      "July 2008 - Apr 2015\n",
      "## Competitions\n",
      "- ¬∑ CODING AT GMOCS-2K18 2K19 : secured 1st position in a one-day national level technical symposium organized by MITS,department of CSE ( Mar 2019 ).\n",
      "- ¬∑ CODING AT ECLECTICA-2019 : secured 2nd position in a one-day national level technical symposium organized by MITS ,department of ECE (April 2019).\n",
      "- ¬∑ PRATIBHA AWARD-2015 : Selected for prathibha award scholarship for securing highest grade in SSC Examination 2015.\n",
      "- ¬∑ 42nd MATHEMATICAL OLYMPIAD-2016 : Secured 6th position in the 42nd mathematical olympiad held in 2014 throughout the state of Andhra pradesh at the Mini-Junior/Sub Junior/Junior/Senior/Junior Inter/Senior Inter/Level.\n",
      "## Honors and Awards\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_ = ''\n",
    "for i in clean_text.split('\\n'):\n",
    "    j = i\n",
    "    if j.strip() != '':\n",
    "        text_ += i +'\\n'\n",
    "print(text_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "213c5b19-96c4-4bb5-b6ca-c01ca1bee0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume_chunking_prompt(resume_text: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are an intelligent document parser designed to preprocess resumes for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to intelligently chunk the given raw resume content into well-defined sections for semantic retrieval. Focus on structuring the data meaningfully and consistently so that downstream models can retrieve relevant information efficiently.\n",
    "Specifically, divide the resume into the following labeled sections:\n",
    "1. Personal Details\n",
    "   - Full name, email, phone number, LinkedIn, GitHub, portfolio links, address (if present).\n",
    "2. Education\n",
    "   - Degrees, institutions, years of study, relevant coursework or achievements.\n",
    "3. Work Experience\n",
    "   - Job titles, company names, duration, responsibilities, technologies used, accomplishments.\n",
    "4. Skills\n",
    "   - List of technical, soft, or domain-specific skills.\n",
    "5. Projects\n",
    "   - Project titles, descriptions, tech stack, roles played, outcomes.\n",
    "6. Certifications / Awards / Achievements (optional section)\n",
    "   - Any notable recognitions, certificates, honors.\n",
    "7. Publications / Research (optional section)\n",
    "   - Any academic publications or relevant research contributions.\n",
    "---\n",
    "üîç Guidelines for Output Formatting:\n",
    "- Label each section clearly with headers.\n",
    "- Preserve bullet points, dates, and formatting that help maintain semantic meaning.\n",
    "- If a section is missing or not clearly identifiable, note it as \"Not Found\".\n",
    "- Ensure each chunk is semantically meaningful and self-contained for downstream use in retrieval.\n",
    "- Prefer consistent formatting, such as bullet points or structured lists where appropriate.\n",
    "---\n",
    "‚úÖ Output Format Example:\n",
    "### Personal Details:\n",
    "Name: Jane Doe  \n",
    "Email: jane.doe@email.com  \n",
    "Phone: +1-123-456-7890  \n",
    "LinkedIn: linkedin.com/in/janedoe  \n",
    "GitHub: github.com/janedoe\n",
    "### Education:\n",
    "- B.Sc. in Computer Science, MIT (2016‚Äì2020)  \n",
    "  Relevant Coursework: Algorithms, Machine Learning  \n",
    "### Work Experience:\n",
    "- Software Engineer at Google (2021‚ÄìPresent)  \n",
    "  ‚Ä¢ Built scalable microservices in Go and Python  \n",
    "  ‚Ä¢ Improved system latency by 30%\n",
    "### Skills:\n",
    "Python, Java, Docker, AWS, SQL, Leadership, Agile\n",
    "### Projects:\n",
    "- Resume Parser App  \n",
    "  ‚Ä¢ Built using Python and spaCy  \n",
    "  ‚Ä¢ Extracted structured data from raw resume PDFs\n",
    "### Certifications / Awards / Achievements:\n",
    "- AWS Certified Solutions Architect  \n",
    "- Winner, HackMIT 2019\n",
    "### Publications / Research:\n",
    "Not Found\n",
    "---\n",
    "Now parse the following resume content accordingly:\n",
    "{resume_text} \n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "85e572ac-1262-4f1d-beaf-28edb772635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('scratch/kr.md', 'r')\n",
    "\n",
    "k1 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "421a8de8-e411-426e-a464-d7fed751cee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Koushik M A\n",
      "9742836226  \n",
      "koushikma62@gmail.com\n",
      "linkedin.com/in/koushik-ma-6378001b7  \n",
      "## PROFILE\n",
      "Experienced Data Analytics Consultant with 4+ years in data visualization, product development, and some  exposure to data engineering and data science. Skilled at turning data into clear insights and building easy- to-use visual tools to support business goals. Strong problem-solver with a focus on delivering effective,  data-driven solutions.\n",
      "## PROFESSIONAL EXPERIENCE\n",
      "Mathco (TheMathCompany), \n",
      "Product Engineer - II (Data Visualisation Using Python)\n",
      "- ‚Ä¢ Guided multiple teams through the product development process, focusing on data visualization and  simplifying complex data science methodologies for better understanding.\n",
      "- ‚Ä¢ Collaborated closely with customers to ensure that the products aligned with their business needs and  were easy to interpret.\n",
      "- ‚Ä¢ Successfully converted 2 proof-of-concept (POC) projects into full-time engagements for clients within a  short timeframe, driving significant business growth to our organization.\n",
      "Associate\n",
      "‚Ä¢ Worked with Fortune 500 clients to demonstrate data science methodology results in our product.\n",
      "- ‚Ä¢ Migrated data reading from blob storage to Snowflake and optimized the code, significantly improving  load times and enhancing customer experience.\n",
      "- ‚Ä¢ Created training materials and trained 100+ employees, including managers, on effectively using the  product and developing data visualization screens.\n",
      "Data Analyst\n",
      "‚Ä¢ Tested data science functionalities during the early stages of product development.\n",
      "- ‚Ä¢ Created 100+ demo applications to showcase product capabilities, helping secure client agreements and  convert some into full-time projects for data visualization.\n",
      "## SKILLS\n",
      "## Technologies:\n",
      "Python, SQL, FastAPI, Basic visualisations in  Power BI\n",
      "Libraries/Framework:\n",
      "NumPy, Pandas, Plotly\n",
      "Tools Used:\n",
      "VS Code, Jupyter notebook  \n",
      "PostgreSQL, Snowflake\n",
      "## EDUCATION\n",
      "University Visvesvaraya College of Engineering,  BE in Computer Science &amp; Engineering; 72.07 Aggregate\n",
      "AWARDS\n",
      "Enterprise Value Award\n",
      "Recognized for driving speed and prioritizing innovation by managing full product development without  the need for new UI development.\n",
      "Aug 2016 - Sep 2020\n",
      "Jul 2024 - present\n",
      "Jul 2022 - Jun 2024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_ = ''\n",
    "for i in k1.split('\\n'):\n",
    "    j = i\n",
    "    if j.strip() != '':\n",
    "        text_ += i +'\\n'\n",
    "print(text_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4dfea906-04db-4e70-8c49-28c9fbec018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = get_resume_chunking_prompt(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b153a935-37a9-492f-9b09-b0bf150049ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an intelligent document parser designed to preprocess resumes for a Retrieval-Augmented Generation (RAG) system.\n",
      "Your task is to intelligently chunk the given raw resume content into well-defined sections for semantic retrieval. Focus on structuring the data meaningfully and consistently so that downstream models can retrieve relevant information efficiently.\n",
      "Specifically, divide the resume into the following labeled sections:\n",
      "1. Personal Details\n",
      "   - Full name, email, phone number, LinkedIn, GitHub, portfolio links, address (if present).\n",
      "2. Education\n",
      "   - Degrees, institutions, years of study, relevant coursework or achievements.\n",
      "3. Work Experience\n",
      "   - Job titles, company names, duration, responsibilities, technologies used, accomplishments.\n",
      "4. Skills\n",
      "   - List of technical, soft, or domain-specific skills.\n",
      "5. Projects\n",
      "   - Project titles, descriptions, tech stack, roles played, outcomes.\n",
      "6. Certifications / Awards / Achievements (optional section)\n",
      "   - Any notable recognitions, certificates, honors.\n",
      "7. Publications / Research (optional section)\n",
      "   - Any academic publications or relevant research contributions.\n",
      "---\n",
      "üîç Guidelines for Output Formatting:\n",
      "- Label each section clearly with headers.\n",
      "- Preserve bullet points, dates, and formatting that help maintain semantic meaning.\n",
      "- If a section is missing or not clearly identifiable, note it as \"Not Found\".\n",
      "- Ensure each chunk is semantically meaningful and self-contained for downstream use in retrieval.\n",
      "- Prefer consistent formatting, such as bullet points or structured lists where appropriate.\n",
      "---\n",
      "‚úÖ Output Format Example:\n",
      "### Personal Details:\n",
      "Name: Jane Doe  \n",
      "Email: jane.doe@email.com  \n",
      "Phone: +1-123-456-7890  \n",
      "LinkedIn: linkedin.com/in/janedoe  \n",
      "GitHub: github.com/janedoe\n",
      "### Education:\n",
      "- B.Sc. in Computer Science, MIT (2016‚Äì2020)  \n",
      "  Relevant Coursework: Algorithms, Machine Learning  \n",
      "### Work Experience:\n",
      "- Software Engineer at Google (2021‚ÄìPresent)  \n",
      "  ‚Ä¢ Built scalable microservices in Go and Python  \n",
      "  ‚Ä¢ Improved system latency by 30%\n",
      "### Skills:\n",
      "Python, Java, Docker, AWS, SQL, Leadership, Agile\n",
      "### Projects:\n",
      "- Resume Parser App  \n",
      "  ‚Ä¢ Built using Python and spaCy  \n",
      "  ‚Ä¢ Extracted structured data from raw resume PDFs\n",
      "### Certifications / Awards / Achievements:\n",
      "- AWS Certified Solutions Architect  \n",
      "- Winner, HackMIT 2019\n",
      "### Publications / Research:\n",
      "Not Found\n",
      "---\n",
      "Now parse the following resume content accordingly:\n",
      "## Koushik M A\n",
      "9742836226  \n",
      "koushikma62@gmail.com\n",
      "linkedin.com/in/koushik-ma-6378001b7  \n",
      "## PROFILE\n",
      "Experienced Data Analytics Consultant with 4+ years in data visualization, product development, and some  exposure to data engineering and data science. Skilled at turning data into clear insights and building easy- to-use visual tools to support business goals. Strong problem-solver with a focus on delivering effective,  data-driven solutions.\n",
      "## PROFESSIONAL EXPERIENCE\n",
      "Mathco (TheMathCompany), \n",
      "Product Engineer - II (Data Visualisation Using Python)\n",
      "- ‚Ä¢ Guided multiple teams through the product development process, focusing on data visualization and  simplifying complex data science methodologies for better understanding.\n",
      "- ‚Ä¢ Collaborated closely with customers to ensure that the products aligned with their business needs and  were easy to interpret.\n",
      "- ‚Ä¢ Successfully converted 2 proof-of-concept (POC) projects into full-time engagements for clients within a  short timeframe, driving significant business growth to our organization.\n",
      "Associate\n",
      "‚Ä¢ Worked with Fortune 500 clients to demonstrate data science methodology results in our product.\n",
      "- ‚Ä¢ Migrated data reading from blob storage to Snowflake and optimized the code, significantly improving  load times and enhancing customer experience.\n",
      "- ‚Ä¢ Created training materials and trained 100+ employees, including managers, on effectively using the  product and developing data visualization screens.\n",
      "Data Analyst\n",
      "‚Ä¢ Tested data science functionalities during the early stages of product development.\n",
      "- ‚Ä¢ Created 100+ demo applications to showcase product capabilities, helping secure client agreements and  convert some into full-time projects for data visualization.\n",
      "## SKILLS\n",
      "## Technologies:\n",
      "Python, SQL, FastAPI, Basic visualisations in  Power BI\n",
      "Libraries/Framework:\n",
      "NumPy, Pandas, Plotly\n",
      "Tools Used:\n",
      "VS Code, Jupyter notebook  \n",
      "PostgreSQL, Snowflake\n",
      "## EDUCATION\n",
      "University Visvesvaraya College of Engineering,  BE in Computer Science &amp; Engineering; 72.07 Aggregate\n",
      "AWARDS\n",
      "Enterprise Value Award\n",
      "Recognized for driving speed and prioritizing innovation by managing full product development without  the need for new UI development.\n",
      "Aug 2016 - Sep 2020\n",
      "Jul 2024 - present\n",
      "Jul 2022 - Jun 2024\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b5f24d54-cf16-456e-a00a-6d08d3d84195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised DeadlineExceeded: 504 Deadline Exceeded.\n"
     ]
    }
   ],
   "source": [
    "res1 = llm_gemini.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ae5559c8-8536-4354-ab22-07344842177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Personal Details:\n",
      "Name: Koushik M A\n",
      "Email: koushikma62@gmail.com\n",
      "Phone: 9742836226\n",
      "LinkedIn: linkedin.com/in/koushik-ma-6378001b7\n",
      "\n",
      "### Education:\n",
      "- University Visvesvaraya College of Engineering, BE in Computer Science & Engineering; 72.07 Aggregate (Aug 2016 - Sep 2020)\n",
      "\n",
      "### Work Experience:\n",
      "- Mathco (TheMathCompany), Product Engineer - II (Data Visualisation Using Python) (Jul 2024 - present)\n",
      "  ‚Ä¢ Guided multiple teams through the product development process, focusing on data visualization and simplifying complex data science methodologies for better understanding.\n",
      "  ‚Ä¢ Collaborated closely with customers to ensure that the products aligned with their business needs and were easy to interpret.\n",
      "  ‚Ä¢ Successfully converted 2 proof-of-concept (POC) projects into full-time engagements for clients within a short timeframe, driving significant business growth to our organization.\n",
      "- Mathco (TheMathCompany), Associate (Jul 2022 - Jun 2024)\n",
      "  ‚Ä¢ Worked with Fortune 500 clients to demonstrate data science methodology results in our product.\n",
      "  ‚Ä¢ Migrated data reading from blob storage to Snowflake and optimized the code, significantly improving load times and enhancing customer experience.\n",
      "  ‚Ä¢ Created training materials and trained 100+ employees, including managers, on effectively using the product and developing data visualization screens.\n",
      "- Mathco (TheMathCompany), Data Analyst\n",
      "  ‚Ä¢ Tested data science functionalities during the early stages of product development.\n",
      "  ‚Ä¢ Created 100+ demo applications to showcase product capabilities, helping secure client agreements and convert some into full-time projects for data visualization.\n",
      "\n",
      "### Skills:\n",
      "Technologies: Python, SQL, FastAPI, Basic visualisations in Power BI\n",
      "Libraries/Framework: NumPy, Pandas, Plotly\n",
      "Tools Used: VS Code, Jupyter notebook, PostgreSQL, Snowflake\n",
      "\n",
      "### Projects:\n",
      "Not Found\n",
      "\n",
      "### Certifications / Awards / Achievements:\n",
      "- Enterprise Value Award\n",
      "  Recognized for driving speed and prioritizing innovation by managing full product development without the need for new UI development.\n",
      "\n",
      "### Publications / Research:\n",
      "Not Found\n"
     ]
    }
   ],
   "source": [
    "print(res1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fb3f2c51-b11c-46b9-8eb1-85c27e2c72ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certifications_awards_achievements\n",
      "publications_research\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "resume_info_meta = {  'personal_details': 'personal',\n",
    "                 'education': 'education',\n",
    "                 'work_experience:': 'work experience',\n",
    "                 'skills': 'skills',\n",
    "                 'projects': 'projects',\n",
    "                 'certifications_awards_achievements': ['certifications' ,'awards','achievements'],\n",
    "                 'publications_research': ['publications', 'research']}\n",
    "resume_info_actual = {}\n",
    "for i in res1.content.split('###'):\n",
    "    meta_ = i.strip().split('\\n')[0]\n",
    "    find = \"none\"\n",
    "    for info in resume_info_meta:\n",
    "        if isinstance(resume_info_meta[info],List) :\n",
    "            for key_meta in resume_info_meta[info]:\n",
    "                if key_meta in meta_.lower():\n",
    "                    find = info\n",
    "                    print(find)\n",
    "                    break\n",
    "            if find != \"none\":\n",
    "                break\n",
    "        elif isinstance(resume_info_meta[info],str):\n",
    "            key_meta = resume_info_meta[info]\n",
    "            if key_meta in meta_.lower():\n",
    "                find = info\n",
    "        if find != \"none\":\n",
    "            break\n",
    "    if find in resume_info_meta:\n",
    "        resume_info_actual[find] = i.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a99375ff-8166-486f-b202-5cafb0f71450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal_details ^^^^^^^^^^\n",
      "Personal Details:\n",
      "Name: Koushik M A\n",
      "Email: koushikma62@gmail.com\n",
      "Phone: 9742836226\n",
      "LinkedIn: linkedin.com/in/koushik-ma-6378001b7\n",
      "----------------------------------------------------------------------------------------------------\n",
      "education ^^^^^^^^^^\n",
      "Education:\n",
      "- University Visvesvaraya College of Engineering, BE in Computer Science & Engineering; 72.07 Aggregate (Aug 2016 - Sep 2020)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "work_experience: ^^^^^^^^^^\n",
      "Work Experience:\n",
      "- Mathco (TheMathCompany), Product Engineer - II (Data Visualisation Using Python) (Jul 2024 - present)\n",
      "  ‚Ä¢ Guided multiple teams through the product development process, focusing on data visualization and simplifying complex data science methodologies for better understanding.\n",
      "  ‚Ä¢ Collaborated closely with customers to ensure that the products aligned with their business needs and were easy to interpret.\n",
      "  ‚Ä¢ Successfully converted 2 proof-of-concept (POC) projects into full-time engagements for clients within a short timeframe, driving significant business growth to our organization.\n",
      "- Mathco (TheMathCompany), Associate (Jul 2022 - Jun 2024)\n",
      "  ‚Ä¢ Worked with Fortune 500 clients to demonstrate data science methodology results in our product.\n",
      "  ‚Ä¢ Migrated data reading from blob storage to Snowflake and optimized the code, significantly improving load times and enhancing customer experience.\n",
      "  ‚Ä¢ Created training materials and trained 100+ employees, including managers, on effectively using the product and developing data visualization screens.\n",
      "- Mathco (TheMathCompany), Data Analyst\n",
      "  ‚Ä¢ Tested data science functionalities during the early stages of product development.\n",
      "  ‚Ä¢ Created 100+ demo applications to showcase product capabilities, helping secure client agreements and convert some into full-time projects for data visualization.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "skills ^^^^^^^^^^\n",
      "Skills:\n",
      "Technologies: Python, SQL, FastAPI, Basic visualisations in Power BI\n",
      "Libraries/Framework: NumPy, Pandas, Plotly\n",
      "Tools Used: VS Code, Jupyter notebook, PostgreSQL, Snowflake\n",
      "----------------------------------------------------------------------------------------------------\n",
      "projects ^^^^^^^^^^\n",
      "Projects:\n",
      "Not Found\n",
      "----------------------------------------------------------------------------------------------------\n",
      "certifications_awards_achievements ^^^^^^^^^^\n",
      "Certifications / Awards / Achievements:\n",
      "- Enterprise Value Award\n",
      "  Recognized for driving speed and prioritizing innovation by managing full product development without the need for new UI development.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "publications_research ^^^^^^^^^^\n",
      "Publications / Research:\n",
      "Not Found\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in resume_info_actual:\n",
    "    print(i,'^'*10)\n",
    "    print(resume_info_actual[i])\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "aec89a48-fa0b-4d1a-aecf-f545fa60a97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/kr.json', 'w') as f:\n",
    "    json.dump(resume_info_actual, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bea41107-7bd2-4cee-8ddb-c97e7ef62a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'personal_details': 'Personal Details:\\nName: Koushik M A\\nEmail: koushikma62@gmail.com\\nPhone: 9742836226\\nLinkedIn: linkedin.com/in/koushik-ma-6378001b7',\n",
       " 'education': 'Education:\\n- University Visvesvaraya College of Engineering, BE in Computer Science & Engineering; 72.07 Aggregate (Aug 2016 - Sep 2020)',\n",
       " 'work_experience:': 'Work Experience:\\n- Mathco (TheMathCompany), Product Engineer - II (Data Visualisation Using Python) (Jul 2024 - present)\\n  ‚Ä¢ Guided multiple teams through the product development process, focusing on data visualization and simplifying complex data science methodologies for better understanding.\\n  ‚Ä¢ Collaborated closely with customers to ensure that the products aligned with their business needs and were easy to interpret.\\n  ‚Ä¢ Successfully converted 2 proof-of-concept (POC) projects into full-time engagements for clients within a short timeframe, driving significant business growth to our organization.\\n- Mathco (TheMathCompany), Associate (Jul 2022 - Jun 2024)\\n  ‚Ä¢ Worked with Fortune 500 clients to demonstrate data science methodology results in our product.\\n  ‚Ä¢ Migrated data reading from blob storage to Snowflake and optimized the code, significantly improving load times and enhancing customer experience.\\n  ‚Ä¢ Created training materials and trained 100+ employees, including managers, on effectively using the product and developing data visualization screens.\\n- Mathco (TheMathCompany), Data Analyst\\n  ‚Ä¢ Tested data science functionalities during the early stages of product development.\\n  ‚Ä¢ Created 100+ demo applications to showcase product capabilities, helping secure client agreements and convert some into full-time projects for data visualization.',\n",
       " 'skills': 'Skills:\\nTechnologies: Python, SQL, FastAPI, Basic visualisations in Power BI\\nLibraries/Framework: NumPy, Pandas, Plotly\\nTools Used: VS Code, Jupyter notebook, PostgreSQL, Snowflake',\n",
       " 'projects': 'Projects:\\nNot Found',\n",
       " 'certifications_awards_achievements': 'Certifications / Awards / Achievements:\\n- Enterprise Value Award\\n  Recognized for driving speed and prioritizing innovation by managing full product development without the need for new UI development.',\n",
       " 'publications_research': 'Publications / Research:\\nNot Found'}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open('data/kr.json'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
