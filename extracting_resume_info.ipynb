{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33059ef-30ce-456d-8503-b57e246280c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb75678-8f3a-475b-a424-8fd5f5a91426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, EmailStr, Field\n",
    "from typing import Optional\n",
    "\n",
    "class PersonalDetails(BaseModel):\n",
    "    name: Optional[str] = Field(\n",
    "        None, description=\"Full name of the candidate.\"\n",
    "    )\n",
    "    email: Optional[EmailStr] = Field(\n",
    "        None, description=\"Candidate's primary email address.\"\n",
    "    )\n",
    "    mobile: Optional[str] = Field(\n",
    "        None, description=\"Candidate's mobile phone number including country code if applicable.\"\n",
    "    )\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class (BaseModel):\n",
    "    title: Optional[str] = Field(\n",
    "        None, description=\"Title or name of the project.\"\n",
    "    )\n",
    "    description: Optional[str] = Field(\n",
    "        None, description=\"Short overview or summary of the project.\"\n",
    "    )\n",
    "    technologies: List[str] = Field(\n",
    "        default_factory=list, description=\"List of technologies or tools used in the project.\"\n",
    "    )\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class WorkExperience(BaseModel):\n",
    "    company: Optional[str] = Field(\n",
    "        None, description=\"Name of the company where the candidate worked.\"\n",
    "    )\n",
    "    title: Optional[str] = Field(\n",
    "        None, description=\"Job title held at the company.\"\n",
    "    )\n",
    "    duration: Optional[str] = Field(\n",
    "        None, description=\"Employment period (e.g., 'Jan 2018 - Dec 2020').\"\n",
    "    )\n",
    "    responsibilities: Optional[str] = Field(\n",
    "        None, description=\"Brief description of the candidate's responsibilities or achievements in the role.\"\n",
    "    )\n",
    "    projects: List[Project] = Field(\n",
    "            default_factory=list, description=\"A list of projects the candidate has worked on\" \n",
    "    )\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Skills(BaseModel):\n",
    "    skills: List[str] = Field(\n",
    "        default_factory=list, description=\"List of the candidate's technical and soft skills.\"\n",
    "    )\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Education(BaseModel):\n",
    "    institution: Optional[str] = Field(\n",
    "        None, description=\"Name of the educational institution.\"\n",
    "    )\n",
    "    degree: Optional[str] = Field(\n",
    "        None, description=\"Degree or certification earned by the candidate.\"\n",
    "    )\n",
    "    year: Optional[str] = Field(\n",
    "        None, description=\"Time period of attendance or graduation year.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9657fbd9-127a-4451-b66e-e78e6bf5dba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Details:\n",
      "Name: Mohan Reddy Pallavula\n",
      "Email: mohanreddy.pmg@gmail.com\n",
      "Phone: +91-8309913459\n",
      "LinkedIn: linkedin/mohanreddypallavula\n",
      "GitHub: github/mohanreddypallavula\n",
      "====================================================================================================\n",
      "Education:\n",
      "- Bachelor of Technology - Computer Science and Engineering; GPA: 8.0\n",
      "  JNTUA Engineering College, Kalikiri, AP, India (July 2017 - June 2021)\n",
      "- Intermediate - MPC; Per: 96.7\n",
      "  Sri Chaitanya Junior College, Tirupati, AP, India (July 2015 - June 2017)\n",
      "- AP Residential School, Gyaram Palli, AP, India\n",
      "  SSC ; GPA: 9.5 (July 2008 - Apr 2015)\n",
      "====================================================================================================\n",
      "Work Experience:\n",
      "- AI Engineer at Matdun Labs India Pvt. Ltd (Remote, Dec 2021 - Present)\n",
      "  • Face Recognition system : Developed an advanced face recognition system utilizing SOTA based models for detecting and recognition the face and deployed on edge device (jetson nano) and kubernetes cluster (Nvidia Gpus). Optimized the models using tensorrt to reduce latency and Used Nvidia Triton server for dynamic batching, GPU and CPU optimization, and robust scalability, ensuring efficient and high-performance inference for deployed AI models. Tech: Tensorrt , Nvidia Jetson Nano , Kubernetes , Docker , FastAPI , Django , Web sockets , Grpc ,Pytorch , Opencv , Scikit-learn , PostgresSQL , Azure blob , GIT\n",
      "  • Video Analytics System : Delevoped an advanced AI-powered solution designed for real-time monitoring and analysis of video streams. It offers features such as person tracking, which enables precise identification and movement analysis, and person analytics, including metrics like waiting time and behavior patterns. The system incorporates specialized detection capabilities, such as weapon detection for enhanced security and fall detection for safety monitoring in environments. Tech: Tensorrt , Kubernetes , Docker , FastAPI , Redis , Grpc , Pytorch Lightning , Litdata , Opencv , Scikit-learn , Nvidia Trition Server , Nvidia deepstream , Pytorch , MLflow , GIT , PostgresSQL , Azure blob\n",
      "  • Generate Description based on activities : Automatically generate detailed and contextually accurate descriptions of activities. We used few shot prompting and llama 3 model to develop the system. Tech: LitGPT , pytorch lightning , Litserve , FastAPI , Redis , QLora , Azure blob , Langchain , Langgraph , Azure Cloud , OpenAI API(GPT-4o) , Llama 3 , RAG\n",
      "- ML Intern at Capillary Technologies Pvt Ltd (Remote, June 2021 - Nov 2021)\n",
      "  • Smart Store : Developed an advanced deep learning to detect the person age group , identifies fashion types, such as specific dress styles. Developed person tracking to analyze customer behavior within the store. By tracking where individuals spend their time, the system provides insights into high-traffic areas and zones of interest. Tech: Deep Learning , AWS , Computer Vision , Nodejs , Docker , GIT\n",
      "- ML Intern at Appcilious Pvt. Ltd (Remote, Sep 2020 - Nov 2020)\n",
      "  • Fake News Classification : Developed an advanced text analysis techniques to accurately identify and categorize misinformation. It employs TF-IDF for initial text feature extraction, Word2Vec for capturing semantic word relationships, and transformer-based embeddings, specifically the Universal Sentence Encoder, for deep contextual understanding. By combining these methods, the system enhances its ability to discern fake news from credible sources. Tech: Machine Learning , NLTK , Scikit-learn , Tf-idf , Deep learning , word2vec.\n",
      "====================================================================================================\n",
      "Skills:\n",
      "- Core Expertise : Data Science, Machine Learning, Deep Learning, Natural Language Processing (NLP), Computer Vision, Predictive Modeling, Decision Analytics, Large Language Models (LLMs), Generative AI\n",
      "- Languages : Python, C , C++\n",
      "- Frameworks : Pytorch , Scikit-learn , FastAPI , Django, Flask , Nvidia Deepestream , Nvidia Triton Server , Langgraph , Langchain , Litgpt , Litdata , Pytorch lightning , Huggingface libraries ( Transformers , peft)\n",
      "- Tools : Kubernetes, Docker, GIT, PostgreSQL, Redis , Gitlab CI-CD\n",
      "- Platforms : Linux, Web, Windows, Nvidia Jetson Nano, Raspberry , Azure , Basic Aws\n",
      "- Soft Skills : Leadership, Time Management\n",
      "====================================================================================================\n",
      "Projects:\n",
      "- Implemented Tinyllama from scratch in pytorch : Built a lightweight version of the LLaMA language model from the ground up using PyTorch, focusing on replicating core transformer architecture with an emphasis on model efficiency and size reduction. Loaded the pretrained weights into our architecture and implemented inference pipeline. Implemented client and server using grpc python. Tech: PyTorch, Transformers, Attention Mechanisms, Deep Learning , KV cache , Grpc , LLM, GenAI .\n",
      "- Image Captioning using Deep Learning (NLP, CV) : Developed an advanced image captioning system using an encoder-decoder architecture to automatically generate coherent and contextually relevant textual descriptions from images. Tech: Python, Tensorflow, Streamlit , and Deep Learning\n",
      "- Semantic search engine on stackoverflow python data (NLP) : Developed a robust semantic search engine designed to enhance search accuracy and relevance within StackOverflow Python data by leveraging advanced natural language processing techniques. Tech: Python, Natural Language Processing (NLP), Word Embeddings, TF-IDF, Transformer Models, Elasticsearch, Scikit-learn, and Pandas.\n",
      "- Face Recognition using Deep Learning : Developed an advanced face recognition system utilizing deep learning techniques to accurately identify and verify individuals from images, with applications in security, identity verification. Tech: Python, Pytorch, Convolutional Neural Networks (CNN), OpenCV, Scikit-learn, TensorRT , Nvidia Jetson nano , fastapi , and Web sockets.\n",
      "====================================================================================================\n",
      "Certifications / Awards / Achievements:\n",
      "- CODING AT GMOCS-2K18 2K19 : secured 1st position in a one-day national level technical symposium organized by MITS,department of CSE ( Mar 2019 ).\n",
      "- CODING AT ECLECTICA-2019 : secured 2nd position in a one-day national level technical symposium organized by MITS ,department of ECE (April 2019).\n",
      "- PRATIBHA AWARD-2015 : Selected for prathibha award scholarship for securing highest grade in SSC Examination 2015.\n",
      "- 42nd MATHEMATICAL OLYMPIAD-2016 : Secured 6th position in the 42nd mathematical olympiad held in 2014 throughout the state of Andhra pradesh at the Mini-Junior/Sub Junior/Junior/Senior/Junior Inter/Senior Inter/Level.\n",
      "====================================================================================================\n",
      "Publications / Research:\n",
      "Not Found\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = 'data/mr.json'\n",
    "\n",
    "with open(file_path,'r') as f:\n",
    "    resume_info = json.load(f)\n",
    "for meta in resume_info:\n",
    "    #print(meta)\n",
    "    print(resume_info[meta])\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa50cb49-c431-4e13-b6e2-db81a6dccc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['personal_details', 'education', 'work_experience:', 'skills', 'projects', 'certifications_awards_achievements', 'publications_research'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "923443b3-15fc-43ed-8745-3b800940bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import *\n",
    "\n",
    "msg_input = work_experience_prompt.format(resume_content=resume_info['work_experience:']) + '\\n' + resume_template_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13041e0a-31ac-43fc-b956-bcd569975653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a resume document in plain text format. Your task is to extract structured candidate details from the resume.\n",
      "Resume Content:\n",
      "Work Experience:\n",
      "- AI Engineer at Matdun Labs India Pvt. Ltd (Remote, Dec 2021 - Present)\n",
      "  • Face Recognition system : Developed an advanced face recognition system utilizing SOTA based models for detecting and recognition the face and deployed on edge device (jetson nano) and kubernetes cluster (Nvidia Gpus). Optimized the models using tensorrt to reduce latency and Used Nvidia Triton server for dynamic batching, GPU and CPU optimization, and robust scalability, ensuring efficient and high-performance inference for deployed AI models. Tech: Tensorrt , Nvidia Jetson Nano , Kubernetes , Docker , FastAPI , Django , Web sockets , Grpc ,Pytorch , Opencv , Scikit-learn , PostgresSQL , Azure blob , GIT\n",
      "  • Video Analytics System : Delevoped an advanced AI-powered solution designed for real-time monitoring and analysis of video streams. It offers features such as person tracking, which enables precise identification and movement analysis, and person analytics, including metrics like waiting time and behavior patterns. The system incorporates specialized detection capabilities, such as weapon detection for enhanced security and fall detection for safety monitoring in environments. Tech: Tensorrt , Kubernetes , Docker , FastAPI , Redis , Grpc , Pytorch Lightning , Litdata , Opencv , Scikit-learn , Nvidia Trition Server , Nvidia deepstream , Pytorch , MLflow , GIT , PostgresSQL , Azure blob\n",
      "  • Generate Description based on activities : Automatically generate detailed and contextually accurate descriptions of activities. We used few shot prompting and llama 3 model to develop the system. Tech: LitGPT , pytorch lightning , Litserve , FastAPI , Redis , QLora , Azure blob , Langchain , Langgraph , Azure Cloud , OpenAI API(GPT-4o) , Llama 3 , RAG\n",
      "- ML Intern at Capillary Technologies Pvt Ltd (Remote, June 2021 - Nov 2021)\n",
      "  • Smart Store : Developed an advanced deep learning to detect the person age group , identifies fashion types, such as specific dress styles. Developed person tracking to analyze customer behavior within the store. By tracking where individuals spend their time, the system provides insights into high-traffic areas and zones of interest. Tech: Deep Learning , AWS , Computer Vision , Nodejs , Docker , GIT\n",
      "- ML Intern at Appcilious Pvt. Ltd (Remote, Sep 2020 - Nov 2020)\n",
      "  • Fake News Classification : Developed an advanced text analysis techniques to accurately identify and categorize misinformation. It employs TF-IDF for initial text feature extraction, Word2Vec for capturing semantic word relationships, and transformer-based embeddings, specifically the Universal Sentence Encoder, for deep contextual understanding. By combining these methods, the system enhances its ability to discern fake news from credible sources. Tech: Machine Learning , NLTK , Scikit-learn , Tf-idf , Deep learning , word2vec.\n",
      "Extract Work Experience:\n",
      "Review the provided resume text and extract information about the candidate’s work experience. For each position, capture the following details:\n",
      " - Company: Name of the organization.\n",
      " - Title: Job title held by the candidate.\n",
      " - Duration: Employment period (for example, \"Jan 2018 - Dec 2020\").\n",
      " - Responsibilities: A brief description of the candidate’s responsibilities or achievements for that role.\n",
      " - Projects: extract the details of projects the candidate has worked on the company. For each project, capture:\n",
      "     - Title: The project's name.\n",
      "     - Description: A short summary of the project.\n",
      "     - Technologies: A list of technologies, tools, or frameworks used.\n",
      "Return the details as a JSON array of work experience objects. If no work experience is listed, return an empty array.\n",
      "Instructions:\n",
      "Read the resume content provided below.\n",
      " - Extract the information for each section, even if one or more sections are missing. For missing sections, return an empty list or null (as applicable).\n",
      " - Format your output as a JSON object that follows the structure defined by the attached Pydantic model.\n",
      " - Do not include any additional keys outside of those specified in the model.\n"
     ]
    }
   ],
   "source": [
    "print(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f55117-a237-4719-9da2-db5c7161954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\n\\n</think>\\n\\nHello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today? 😊\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm_ollama = ChatOllama(temperature=0, model=\"deepseek-r1:1.5b\")\n",
    "llm_ollama.invoke(\"Hello, how are you?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdcc3db6-3eab-4523-88d6-71828ab96e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ollama = llm_ollama.invoke(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5dd457f-9c51-4f24-91d4-0862ec6ce197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Company': 'Matdun Labs India Pvt. Ltd', 'Title': 'AI Engineer', 'Duration': 'Remote, Dec 2021 - Present', 'Responsibilities': ['Developed an advanced face recognition system utilizing SOTA based models for detecting and recognition the face and deployed on edge device (jetson nano) and Kubernetes cluster (Nvidia Gpus). Optimized the models using tensorrt to reduce latency and Used Nvidia Triton server for dynamic batching, GPU and CPU optimization, and robust scalability, ensuring efficient and high-performance inference for deployed AI models.', 'Developed an advanced AI-powered solution designed for real-time monitoring and analysis of video streams. It offers features such as person tracking, which enables precise identification and movement analysis, and person analytics, including metrics like waiting time and behavior patterns. The system incorporates specialized detection capabilities, such as weapon detection for enhanced security and fall detection for safety monitoring in environments.', 'Developed an advanced text analysis techniques to accurately identify and categorize misinformation. It employs TF-IDF for initial text feature extraction, Word2Vec for capturing semantic word relationships, and transformer-based embeddings, specifically the Universal Sentence Encoder, for deep contextual understanding.'], 'Projects': [{'Title': 'Face Recognition System', 'Description': 'Developed an advanced face recognition system utilizing SOTA based models for detecting and recognition the face and deployed on edge device (jetson nano) and Kubernetes cluster (Nvidia Gpus). Optimized the models using tensorrt to reduce latency and Used Nvidia Triton server for dynamic batching, GPU and CPU optimization, and robust scalability, ensuring efficient and high-performance inference for deployed AI models.', 'Technologies': ['TensorRT', 'Nvidia Jetson Nano', 'Kubernetes', 'Docker', 'FastAPI', 'Django', 'Web sockets', 'Grpc', 'Pytorch', 'Opencv', 'Scikit-learn', 'PostgresSQL', 'Azure blob', 'Git']}, {'Title': 'Video Analytics System', 'Description': 'Developed an advanced AI-powered solution for real-time monitoring and analysis. It offers features such as person tracking, which enables precise identification and movement analysis, and person analytics, including metrics like waiting time and behavior patterns.', 'Technologies': ['TensorRT', 'Kubernetes', 'FastAPI', 'Redis', 'Grpc', 'Pytorch Lightning', 'Litdata', 'Opencv', 'Scikit-learn', 'Nvidia Trition Server', 'Nvidia deepstream', 'Pytorch', 'MLflow', 'Git', 'PostgresSQL', 'Azure blob']}, {'Title': 'Generate Description based on activities', 'Description': 'Automatically generate detailed and contextually accurate descriptions of activities.', 'Technologies': ['LitGPT', 'pytorch lightning', 'Litserve', 'FastAPI', 'Redis', 'QLora', 'Azure blob', 'Langchain', 'Langgraph', 'Azure Cloud', 'OpenAI API(GPT-4o)', 'Llama 3', 'RAG']}]}, {'Company': 'Capillary Technologies Pvt. Ltd', 'Title': 'ML Intern', 'Duration': 'Remote, June 2021 - Nov 2021', 'Responsibilities': ['Developed an AI to detect person age group and fashion type using person tracking.', 'Summary: Analyzes customer behavior in the store.'], 'Projects': [{'Title': 'Smart Store', 'Description': 'Developed an AI to detect person age group and fashion type. The system uses person tracking to analyze customer behavior in the store.', 'Technologies': ['Deep Learning', 'AWS', 'Computer Vision', 'Nodejs', 'Docker', 'Git']}, {'Title': 'Fake News Classification', 'Description': 'Used TF-IDF, Word2Vec, and Transformer embeddings for fake news detection. The system classifies fake news based on text analysis techniques.', 'Technologies': ['Machine Learning', 'NLTK', 'Scikit-learn', 'Tf-idf', 'Deep learning', 'word2vec']}]}, {'Company': 'Appcilious Pvt. Ltd', 'Title': 'ML Intern', 'Duration': 'Remote, Sep 2020 - Nov 2020', 'Responsibilities': ['Developed text analysis techniques for misinformation classification.'], 'Projects': [{'Title': 'Fake News Classification', 'Description': 'Developed an advanced text analysis technique using TF-IDF, Word2Vec, and Transformer embeddings for fake news detection. The system classifies fake news based on text analysis techniques.', 'Technologies': ['Machine Learning', 'NLTK', 'Scikit-learn', 'Tf-idf', 'Deep learning', 'word2vec']}]}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object = WorkExperience)\n",
    "\n",
    "json_out = parser.invoke(res_ollama)\n",
    "\n",
    "print(json_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73538f53-2408-4dc4-9645-22adbf9867df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Company': 'Capillary Technologies Pvt. Ltd',\n",
       " 'Title': 'ML Intern',\n",
       " 'Duration': 'Remote, June 2021 - Nov 2021',\n",
       " 'Responsibilities': ['Developed an AI to detect person age group and fashion type using person tracking.',\n",
       "  'Summary: Analyzes customer behavior in the store.'],\n",
       " 'Projects': [{'Title': 'Smart Store',\n",
       "   'Description': 'Developed an AI to detect person age group and fashion type. The system uses person tracking to analyze customer behavior in the store.',\n",
       "   'Technologies': ['Deep Learning',\n",
       "    'AWS',\n",
       "    'Computer Vision',\n",
       "    'Nodejs',\n",
       "    'Docker',\n",
       "    'Git']},\n",
       "  {'Title': 'Fake News Classification',\n",
       "   'Description': 'Used TF-IDF, Word2Vec, and Transformer embeddings for fake news detection. The system classifies fake news based on text analysis techniques.',\n",
       "   'Technologies': ['Machine Learning',\n",
       "    'NLTK',\n",
       "    'Scikit-learn',\n",
       "    'Tf-idf',\n",
       "    'Deep learning',\n",
       "    'word2vec']}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf182ed0-2804-440b-9324-869f708cffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_input = projects_prompt.format(resume_content=resume_info['projects']) + '\\n' + resume_template_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5873baa3-dd44-476f-bc96-8c7166109fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a resume document in plain text format. Your task is to extract structured candidate details from the resume.\n",
      "Resume Content:\n",
      "Projects:\n",
      "- Implemented Tinyllama from scratch in pytorch : Built a lightweight version of the LLaMA language model from the ground up using PyTorch, focusing on replicating core transformer architecture with an emphasis on model efficiency and size reduction. Loaded the pretrained weights into our architecture and implemented inference pipeline. Implemented client and server using grpc python. Tech: PyTorch, Transformers, Attention Mechanisms, Deep Learning , KV cache , Grpc , LLM, GenAI .\n",
      "- Image Captioning using Deep Learning (NLP, CV) : Developed an advanced image captioning system using an encoder-decoder architecture to automatically generate coherent and contextually relevant textual descriptions from images. Tech: Python, Tensorflow, Streamlit , and Deep Learning\n",
      "- Semantic search engine on stackoverflow python data (NLP) : Developed a robust semantic search engine designed to enhance search accuracy and relevance within StackOverflow Python data by leveraging advanced natural language processing techniques. Tech: Python, Natural Language Processing (NLP), Word Embeddings, TF-IDF, Transformer Models, Elasticsearch, Scikit-learn, and Pandas.\n",
      "- Face Recognition using Deep Learning : Developed an advanced face recognition system utilizing deep learning techniques to accurately identify and verify individuals from images, with applications in security, identity verification. Tech: Python, Pytorch, Convolutional Neural Networks (CNN), OpenCV, Scikit-learn, TensorRT , Nvidia Jetson nano , fastapi , and Web sockets.\n",
      "Extract Projects:\n",
      "From the resume text, extract the details of projects the candidate has worked on. For each project, capture:\n",
      " - Title: The project's name.\n",
      " - Description: A short summary of the project.\n",
      " - Technologies: A list of technologies, tools, or frameworks used.\n",
      "Present your findings as a JSON array. If the candidate has no projects listed, return an empty array.\n",
      "Instructions:\n",
      "Read the resume content provided below.\n",
      " - Extract the information for each section, even if one or more sections are missing. For missing sections, return an empty list or null (as applicable).\n",
      " - Format your output as a JSON object that follows the structure defined by the attached Pydantic model.\n",
      " - Do not include any additional keys outside of those specified in the model.\n"
     ]
    }
   ],
   "source": [
    "print(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7172411b-6e80-4bc3-8705-efa5d70749df",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ollama = llm_ollama.invoke(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81cf8018-c0d0-4cb7-bbbf-afa158a44a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Implementation of Tinyllama from scratch', 'description': 'Built a lightweight version of the LLaMA language model from the ground up using PyTorch, focusing on replicating core transformer architecture with an emphasis on model efficiency and size reduction.Loaded the pretrained weights into our architecture and implemented inference pipeline.', 'technologies': ['PyTorch', 'Transformers', 'Attention Mechanisms', 'Deep Learning']}\n",
      "{'title': 'Image Captioning using Deep Learning', 'description': 'Developed an advanced image captioning system using an encoder-decoder architecture to automatically generate coherent and contextually relevant textual descriptions from images.', 'technologies': ['Python', 'TensorFlow', 'Streamlit', 'Deep Learning']}\n",
      "{'title': 'Semantic search engine on stackoverflow python data', 'description': 'Developed a robust semantic search engine designed to enhance search accuracy and relevance within StackOverflow Python data by leveraging advanced natural language processing techniques.', 'technologies': ['Python', 'Natural Language Processing (NLP)', 'Word Embeddings', 'TF-IDF', 'Transformer Models', 'Elastix', 'Scikit-learn', 'Pandas']}\n",
      "{'title': 'Face Recognition using Deep Learning', 'description': 'Developed an advanced face recognition system utilizing deep learning techniques to accurately identify and verify individuals from images, with applications in security, identity verification.', 'technologies': ['Python', 'PyTorch', 'Convolutional Neural Networks (CNN)', 'OpenCV', 'Scikit-learn', 'TensorRT', ' Nvidia Jetson nano', 'FastAPI', 'Web sockets']}\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputParser(pydantic_object = Project)\n",
    "\n",
    "json_out = parser.invoke(res_ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9e7f6e5-cbe0-4deb-aad0-73bb7952ba11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Implementation of Tinyllama from scratch',\n",
       " 'description': 'Built a lightweight version of the LLaMA language model from the ground up using PyTorch, focusing on replicating core transformer architecture with an emphasis on model efficiency and size reduction.Loaded the pretrained weights into our architecture and implemented inference pipeline.',\n",
       " 'technologies': ['PyTorch',\n",
       "  'Transformers',\n",
       "  'Attention Mechanisms',\n",
       "  'Deep Learning']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ac790a1-d370-4ed7-be77-4f9519fa8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ResumeSkills(BaseModel):\n",
    "    technical_skills: List[str]\n",
    "    soft_skills: List[str]\n",
    "    domain_specific_skills: List[str]\n",
    "    tools_and_platforms: List[str]\n",
    "    languages: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d42656ef-5e75-44b2-acf2-83d01c7ac3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_input = skills_prompt.format(resume_content=resume_info['skills']) + '\\n' + resume_template_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e3109a9-89ee-4451-b2df-61711afcc23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional resume analyzer.\n",
      "Your task is to extract all **skills** mentioned in the given resume. Group them into the following categories:\n",
      "\n",
      "1. **Technical Skills** – programming languages, frameworks, libraries, dev tools, databases, cloud services, etc.\n",
      "2. **Soft Skills** – communication, leadership, time management, problem-solving, etc.\n",
      "3. **Domain-Specific Skills** – subject matter expertise or industry-specific knowledge (e.g., fintech, robotics, bioinformatics).\n",
      "4. **Tools & Platforms** – IDEs, productivity tools, software platforms (e.g., Git, JIRA, Figma).\n",
      "5. **Languages** – spoken languages (e.g., English, Spanish, German).\n",
      "\n",
      "Return the output as a structured JSON object with each skill category containing a list of strings. Do not include explanations or extra formatting — only valid JSON.\n",
      "use these names for json keys technical_skills , soft_skills , domain_specific_skills , tools_and_platforms , languages\n",
      "If a skill category is not present, return an empty list for that category.\n",
      "Now extract the skills from the following resume:\n",
      "Skills:\n",
      "- Core Expertise : Data Science, Machine Learning, Deep Learning, Natural Language Processing (NLP), Computer Vision, Predictive Modeling, Decision Analytics, Large Language Models (LLMs), Generative AI\n",
      "- Languages : Python, C , C++\n",
      "- Frameworks : Pytorch , Scikit-learn , FastAPI , Django, Flask , Nvidia Deepestream , Nvidia Triton Server , Langgraph , Langchain , Litgpt , Litdata , Pytorch lightning , Huggingface libraries ( Transformers , peft)\n",
      "- Tools : Kubernetes, Docker, GIT, PostgreSQL, Redis , Gitlab CI-CD\n",
      "- Platforms : Linux, Web, Windows, Nvidia Jetson Nano, Raspberry , Azure , Basic Aws\n",
      "- Soft Skills : Leadership, Time Management\n",
      "\n",
      "Instructions:\n",
      "Read the resume content provided below.\n",
      " - Extract the information for each section, even if one or more sections are missing. For missing sections, return an empty list or null (as applicable).\n",
      " - Format your output as a JSON object that follows the structure defined by the attached Pydantic model.\n",
      " - Do not include any additional keys outside of those specified in the model.\n"
     ]
    }
   ],
   "source": [
    "print(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd85af2b-5548-41b3-b7fe-60471efc9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ollama = llm_ollama.invoke(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59d7c58e-7e29-480d-9a56-c5c14062f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'technical_skills': ['Data Science', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Predictive Modeling', 'Decision Analytics', 'Large Language Models (LLMs)', 'Generative AI', 'Python', 'C', 'C++', 'PyTorch', 'Scikit-learn', 'FastAPI', 'Django', 'Flask', 'Nvidia Deepestream', 'Nvidia Triton Server', 'Langgraph', 'Langchain', 'LitGPT', 'Litdata', 'PyTorch Lightning', 'Huggingface libraries ( Transformers, peft)'], 'soft_skills': ['Leadership', 'Time Management'], 'domain_specific_skills': ['Data Science', 'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision', 'Predictive Modeling', 'Decision Analytics', 'Large Language Models (LLMs)', 'Generative AI'], 'tools_and_platforms': ['Kubernetes', 'Docker', 'Git', 'PostgreSQL', 'Redis', 'Gitlab CI-CD'], 'languages': ['Python', 'C', 'C++']}\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputParser(pydantic_object = ResumeSkills)\n",
    "\n",
    "json_out = parser.invoke(res_ollama)\n",
    "\n",
    "print(json_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dba6fa8f-e6ef-434c-9500-3e8591e157c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'technical_skills': ['Data Science',\n",
       "  'Machine Learning',\n",
       "  'Deep Learning',\n",
       "  'NLP',\n",
       "  'Computer Vision',\n",
       "  'Predictive Modeling',\n",
       "  'Decision Analytics',\n",
       "  'Large Language Models (LLMs)',\n",
       "  'Generative AI',\n",
       "  'Python',\n",
       "  'C',\n",
       "  'C++',\n",
       "  'PyTorch',\n",
       "  'Scikit-learn',\n",
       "  'FastAPI',\n",
       "  'Django',\n",
       "  'Flask',\n",
       "  'Nvidia Deepestream',\n",
       "  'Nvidia Triton Server',\n",
       "  'Langgraph',\n",
       "  'Langchain',\n",
       "  'LitGPT',\n",
       "  'Litdata',\n",
       "  'PyTorch Lightning',\n",
       "  'Huggingface libraries ( Transformers, peft)'],\n",
       " 'soft_skills': ['Leadership', 'Time Management'],\n",
       " 'domain_specific_skills': ['Data Science',\n",
       "  'Machine Learning',\n",
       "  'Deep Learning',\n",
       "  'NLP',\n",
       "  'Computer Vision',\n",
       "  'Predictive Modeling',\n",
       "  'Decision Analytics',\n",
       "  'Large Language Models (LLMs)',\n",
       "  'Generative AI'],\n",
       " 'tools_and_platforms': ['Kubernetes',\n",
       "  'Docker',\n",
       "  'Git',\n",
       "  'PostgreSQL',\n",
       "  'Redis',\n",
       "  'Gitlab CI-CD'],\n",
       " 'languages': ['Python', 'C', 'C++']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e566d92-6d3c-43e8-b487-a970db3e7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_input = education_prompt.format(resume_content=resume_info['education']) + '\\n' + resume_template_instructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e2237bb-945b-4c62-8689-3068761b4e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a resume document in plain text format. Your task is to extract structured candidate details from the resume.\n",
      "Resume Content:\n",
      "Education:\n",
      "- Bachelor of Technology - Computer Science and Engineering; GPA: 8.0\n",
      "  JNTUA Engineering College, Kalikiri, AP, India (July 2017 - June 2021)\n",
      "- Intermediate - MPC; Per: 96.7\n",
      "  Sri Chaitanya Junior College, Tirupati, AP, India (July 2015 - June 2017)\n",
      "- AP Residential School, Gyaram Palli, AP, India\n",
      "  SSC ; GPA: 9.5 (July 2008 - Apr 2015)\n",
      "Extract Education Details:\n",
      "Please extract the candidate’s educational background from the resume text. For each entry, include:\n",
      " - Institution: The name of the school, college, or university.\n",
      " - Degree: The degree or certification earned.\n",
      " - Year: The time period of attendance or the graduation year.\n",
      "Return these details as a JSON array. If the candidate provides no education details, return an empty array.\n",
      "Instructions:\n",
      "Read the resume content provided below.\n",
      " - Extract the information for each section, even if one or more sections are missing. For missing sections, return an empty list or null (as applicable).\n",
      " - Format your output as a JSON object that follows the structure defined by the attached Pydantic model.\n",
      " - Do not include any additional keys outside of those specified in the model.\n"
     ]
    }
   ],
   "source": [
    "print(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9da8444-65bb-493d-af1d-06bb9e6b7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ollama = llm_ollama.invoke(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2093fc44-d5be-4553-9c54-b43ecdfaecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object = Education)\n",
    "\n",
    "json_out = parser.invoke(res_ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4356809-fab4-48f2-8546-e9e20e4b4b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Institution': 'JNTUA Engineering College',\n",
       "  'Degree': 'Bachelor of Technology - Computer Science and Engineering',\n",
       "  'Year': '2021'},\n",
       " {'Institution': 'Sri Chaitanya Junior College',\n",
       "  'Degree': 'Intermediate - MPC',\n",
       "  'Year': '2017'},\n",
       " []]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e16676d6-832a-4f02-919f-41303a656480",
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_prompt = \"\"\"You are an expert resume parser. Your task is to extract all **publications and research** mentioned in the resume and return the result as a **JSON array** of structured objects.\n",
    "Each item should include the following fields (if available):\n",
    "- **title**: Title of the paper or research work\n",
    "- **authors**: List of authors (including the resume owner if named)\n",
    "- **publication_venue**: Journal, conference, or platform where it was published\n",
    "- **year**: Year of publication (integer)\n",
    "- **doi_or_link**: DOI or a direct link to the publication (if available)\n",
    "- **description**: A short summary or abstract (if present)\n",
    "\n",
    "Return only valid JSON. If no publications or research work is found, return an empty list.\n",
    "Now extract this information from the resume below:\n",
    "{resume_content}\n",
    "\"\"\"\n",
    "\n",
    "data_pub = \"\"\"Publications & Research\n",
    "\"Enhancing Image Captioning Using Transformer Networks\"\n",
    "John Doe, Priya Patel, Ankit Sharma\n",
    "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023\n",
    "https://doi.org/10.1109/CVPR.2023.00456\n",
    "Developed a novel transformer-based encoder-decoder architecture that improves caption generation for complex visual scenes.\n",
    "\n",
    "\"Real-Time Object Detection Using YOLOv5 and TensorRT\"\n",
    "John Doe, Rajesh Iyer\n",
    "International Journal of Computer Applications (IJCA), Vol. 185, No. 12, 2022\n",
    "Focused on deploying YOLOv5 models optimized with TensorRT for edge devices, achieving inference speeds of up to 100 FPS.\n",
    "\n",
    "\"Survey on Federated Learning Techniques for Privacy-Preserving AI\"\n",
    "John Doe, Sneha Reddy\n",
    "ArXiv Preprint, 2021\n",
    "https://arxiv.org/abs/2106.12345\n",
    "A comprehensive review of federated learning approaches, covering optimization challenges and privacy guarantees.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, HttpUrl\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class Publication(BaseModel):\n",
    "    title: str\n",
    "    authors: List[str]\n",
    "    publication_venue: Optional[str] = None\n",
    "    year: Optional[int] = None\n",
    "    doi_or_link: Optional[HttpUrl] = None\n",
    "    description: Optional[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba5d3416-6cc3-4475-adcc-c4f0d36226e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert resume parser. Your task is to extract all **publications and research** mentioned in the resume and return the result as a **JSON array** of structured objects.\n",
      "Each item should include the following fields (if available):\n",
      "- **title**: Title of the paper or research work\n",
      "- **authors**: List of authors (including the resume owner if named)\n",
      "- **publication_venue**: Journal, conference, or platform where it was published\n",
      "- **year**: Year of publication (integer)\n",
      "- **doi_or_link**: DOI or a direct link to the publication (if available)\n",
      "- **description**: A short summary or abstract (if present)\n",
      "\n",
      "Return only valid JSON. If no publications or research work is found, return an empty list.\n",
      "Now extract this information from the resume below:\n",
      "Publications & Research\n",
      "\"Enhancing Image Captioning Using Transformer Networks\"\n",
      "John Doe, Priya Patel, Ankit Sharma\n",
      "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023\n",
      "https://doi.org/10.1109/CVPR.2023.00456\n",
      "Developed a novel transformer-based encoder-decoder architecture that improves caption generation for complex visual scenes.\n",
      "\n",
      "\"Real-Time Object Detection Using YOLOv5 and TensorRT\"\n",
      "John Doe, Rajesh Iyer\n",
      "International Journal of Computer Applications (IJCA), Vol. 185, No. 12, 2022\n",
      "Focused on deploying YOLOv5 models optimized with TensorRT for edge devices, achieving inference speeds of up to 100 FPS.\n",
      "\n",
      "\"Survey on Federated Learning Techniques for Privacy-Preserving AI\"\n",
      "John Doe, Sneha Reddy\n",
      "ArXiv Preprint, 2021\n",
      "https://arxiv.org/abs/2106.12345\n",
      "A comprehensive review of federated learning approaches, covering optimization challenges and privacy guarantees.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "msg_input = publications_prompt.format(resume_content=data_pub) #+ '\\n' + resume_template_instructions\n",
    "\n",
    "print(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4c1eed1-a134-4bdf-9cc1-d260b98bac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ollama = llm_ollama.invoke(msg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ac2e882d-9f99-44cb-b6ff-0c833b23340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object = Publication)\n",
    "\n",
    "json_out = parser.invoke(res_ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64fef249-0305-44e8-9c57-d001758454d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Enhancing Image Captioning Using Transformer Networks',\n",
       "  'authors': ['John Doe', 'Priya Patel', 'Ankit Sharma'],\n",
       "  'publication_venue': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)',\n",
       "  'year': 2023,\n",
       "  'doi_or_link': 'https://doi.org/10.1109/CVPR.2023.00456',\n",
       "  'description': 'Improving caption generation for complex visual scenes.'},\n",
       " {'title': 'Developed a novel transformer-based encoder-decoder architecture that improves caption generation for complex visual scenes.',\n",
       "  'authors': ['John Doe', 'Rajesh Iyer'],\n",
       "  'publication_venue': 'International Journal of Computer Applications (IJCA)',\n",
       "  'year': 2022,\n",
       "  'doi_or_link': 'https://doi.org/10.1109/CVPR.2023.00456',\n",
       "  'description': 'Focusing on deploying YOLOv5 models optimized with TensorRT for edge devices, achieving inference speeds of up to 100 FPS.'},\n",
       " {'title': 'Survey on Federated Learning Techniques for Privacy-Preserving AI',\n",
       "  'authors': ['John Doe', 'Sneha Reddy'],\n",
       "  'publication_venue': 'ArXiv Preprint',\n",
       "  'year': 2021,\n",
       "  'doi_or_link': 'https://arxiv.org/abs/2106.12345',\n",
       "  'description': 'A comprehensive review of federated learning approaches, covering optimization challenges and privacy guarantees.'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
